\documentclass{article}[12pt, a4paper]
\topmargin = 15pt

\usepackage{graphics}
\usepackage[top=1.25in, bottom=1.25in, left=44mm, right=44mm]{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

%\VignetteIndexEntry{Downscaling tutorial}
%\vignetteDepends{downscale}

\graphicspath{{figures/}}

\SweaveOpts{png=TRUE}
\SweaveOpts{resolution=100}
\SweaveOpts{keep.source=TRUE}

<<foo,include=FALSE,echo=FALSE>>=

options(width = 60)
set.seed(0)
@

\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Downscaling species occupancy: \\ an introduction and tutorial}
\author{Charles J. Marsh}
\date{\today}
\maketitle

\section{Introduction to downscaling}

In order to assess and manage the status of a species we need to know the abundance of individuals in the population(s) and their changes over time. For the vast majority of species this information is unobtainable, however one important proxy of true abundance and extinction risk is the area occupied by the species. For example, the area of occupancy (AOO) is a little-used measure of conservation status in the IUCN red list (IUCN 2014). Although easier to estimate than true abundance, the difficulty in estimating AOO lies in the extensive sampling required across the full range of the species at a grain size sufficiently fine to give meaningful estimates (and this grain size may vary with taxa, habitat or region). For the majority of species this is still impractical or unfeasible at these grain sizes. However, as we estimate occupancy at increasing grain sizes we increase our confidence in our presence-absence predictions. Such coarse-grain atlas data, generally generated from opportunistic recording over extended periods of time are much more widely available, however, at such coarse grain sizes we also lose resolution in our status estimates as occupancy rates at large grain sizes are less closely correlated with true abundance (Hartley and Kunin 2003).

A solution is to employ the occupancy-area relationship (OAR), that is the increase in the  area occupied by a species increases as grain size increases (Kunin 1998). If the relationship can be described for occupancy at these coarser grain sizes, where confidence is high, then we can extrapolate the occupancy predictions to the fine grain sizes necessary for conservation assessments that are more closely related to the true abundance, distribution and conservation status.

Many models have been proposed to model this geometric relationship, and it appears that no one model consistently provides the best predictions (Azaele et al. 2012, Barwell et al. 2014). This package provides functions for ten commonly applied models, along with functions for preparing coarse-scale data, plotting results, and an ensemble method for running multiple models and averaging their predictions.

\section{Using the downscale package}

The general flow of the \texttt{downscale} package is presented in fig. \ref{fig:Flow}. Ten downscaling models are available (Nachman, power law, logistic, poisson, negative binomial, generalised negative binomial, improved negative binomial, finite negative binomial, Thomas and Hui models). Details of all models can be found in the help files, and in the supplementary information of Barwell et al. 2014. 

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{Flow.png}
\caption{Structure of the \texttt{downscale} package showing all eight functions (yellow) and the three output object classes (orange).}
\label{fig:Flow}
\end{figure}

The user may input three types of data:
\begin{enumerate} \itemsep1pt \parskip0pt 
\item [1)] A data frame of grain sizes (cell area) and occupancies in that order;
\item [2)] A data frame of sample (cell) coordinates and presence-absence data (presence = 1; absence = 0). Column names must be “lon”, “lat”, and “presence”;
\item [3)] A raster layer of presence-absence data (presence = 1; absence = 0; no data = NA).
\end{enumerate}

If the user wishes to carry out downscaling with the Hui model (Hui et al. 2006, 2009) or upgraining of atlas data (and exploration of upgraining thresholds) then the input data must be of type 2 or 3. Table 1 shows the functions to use to achieve desired objectives with regards to input data.

\begin{table}[!h]
\begin{tabular}{| p{3.9cm} | p{3.5cm} | p{4.1cm} |}
\hline
\textbf{Input data type} & \textbf{Objective} & \textbf{Function flow}  \\\hline
Data frame of cell areas and occcupancies & Downscale (excluding Hui model) & \parbox[t]{0.1cm}{ \texttt{downscale}\Rightarrow \ \texttt{predict}\Rightarrow \ \texttt{plot}}
\\\hline
Data frame of cell coordinates and presence-absence data & Downscale (excluding Hui model) & \parbox[t]{0.1cm}{ \texttt{upgrain.threshold}\Rightarrow\ \texttt{upgrain} \Rightarrow\  \texttt{downscale}\Rightarrow\ \texttt{predict}\Rightarrow\ \texttt{plot}}
\\\hline
Raster layer of presence-absence data & Downscale (excluding Hui model)	& \parbox[t]{0.1cm}{ \texttt{upgrain.threshold}\Rightarrow \ \texttt{upgrain}\Rightarrow \ \texttt{downscale}\Rightarrow \ \texttt{predict}\Rightarrow \texttt{plot}}
\\\hline
Data frame of cell coordinates and presence-absence data &	Downscale (including Hui model)	& \parbox[t]{0.1cm}{ \texttt{hui.downscale}\Rightarrow \ \texttt{plot}}
\\\hline
Raster layer of presence-absence data &	Downscale (including Hui model) &	\parbox[t]{0.1cm}{ \texttt{hui.downscale}\Rightarrow \ \texttt{plot}}
\\\hline
Data frame of cell coordinates and presence-absence data &	Ensemble modelling (excluding Hui model) &	\parbox[t]{0.1cm}{\texttt{ensemble.downscale}}
\\\hline
Raster layer of presence-absence data &	Ensemble modelling (with or without Hui model) &	\texttt{upgrain.threshold}\Rightarrow \ \texttt{upgrain}\Rightarrow \ \texttt{ensemble.downscale}}
\\\hline
\end{tabular}
\caption{Flow of functions for different objectives depending on data input type.}
\end{table}


For downscale modelling it is important they check their data for the scale of saturation and endemism. The scale of saturation is the grain size where all cells are occupied (fig. \ref{fig:Saturation}a). The scale of endemism is the grain size where the entire distribution occurs in a single cell (\ref{fig:Saturation}b). All occupancies above these grain sizes should be set to NA, as they are providing no information for the occupancy-area curve. The downscale functions will automatically set these occupancies to NA for modelling purposes, which can lead to not enough scales remaining for downscaling.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{Saturation.png}
\caption{Occupancy-area relationships for two species showing a) the scale of saturation (the grain size at which all cells are occupied – ie occupancy = 1) and b) the scale of endemism (the scale at which only one cell is occupied). Occupancies of all grain sizes above these points should be set to NA.}
\label{fig:Saturation}
\end{figure}

\section{Package tutorial}

First, we must download the downscale package from CRAN.

<<downscale1, echo = TRUE, eval = FALSE, fig = FALSE, strip.white = false>>=
install.packages("downscale")
@

Then load in the library

<<downscale2, echo = TRUE, fig = FALSE, strip.white = false>>=
library("downscale")
@

\subsection{A quick example}
We will start with the simplest example of using the downscaling package, where we already have occupancy data across a number of spatial scales (grain size). In this tutorial, we’ll create some dummy data; a data frame where the first column are the cell areas (grain size) and the proportion of occupancy as the second column:

<<downscale3, echo = TRUE, strip.white = false>>=
occupancy <- data.frame(Cell.area = c(100, 400, 1600, 6400),
                        Occupancy = c(0.23, 0.56, 0.87, 1))
@

Now we use downscale to estimate the model parameters for the logistic model to the data (note: for this type of data input we must also specify the total extent):

<<downscale4, echo = TRUE, strip.white = false>>=
logis.mod <- downscale(occupancies = occupancy,
                       model = "Logis",
                       extent = 320000)
	
## this creates an object of class ‘downscale’
logis.mod
@

Using the modelled parameters from the \texttt{‘downscale’} object we can predict occupancies at finer grain sizes. We will first create a vector of cell sizes (area) to predict. If we include the original cell sizes used for modelling we can also observe the model fit.

<<downscale5, echo = TRUE, strip.white = false>>=
areas.pred <- c(1, 2, 5, 25, 100, 400, 1600, 6400)
logis.pred <- predict(logis.mod,
                      new.areas = areas.pred)

## this creates an object of class ‘predict.downscale’
## occupancy is given as a proportion and area of occupancy (AOO)
logis.pred$predicted
@
<<downscale6, echo = TRUE, fig = TRUE, strip.white = false>>=
## now we can plot the predictions
plot(logis.pred)
@

\subsection{Using atlas data}
For the majority of cases we will only have atlas data that first needs to be upgrained. Read in example atlas data for the UK (in this case a data frame of sample cell coordinates and presence-absence data):

<<downscale7, echo = TRUE, strip.white = false>>=
## if it is not already loaded, load in the package
library(downscale)

data.file <- system.file("extdata", "atlas_data.txt", package = "downscale")
atlas.data <- read.table(data.file, header = TRUE)
@

The data frame must have the column names “lon”, “lat” and “presence”:

<<downscale8, echo = TRUE, strip.white = false>>=
head(atlas.data)
@

The first step is to upgrain the atlas data to calculate occupancy at larger grain sizes than the atlas data – this provides the proportion of occupancy data points to fit the different downscaling models to. Therefore it is important that we fix the extent for all grain sizes, but this means compromising between assigning unsampled cells as absences or excluding sampled cells as No Data. We can explore this trade-off with \texttt{upgrain.threshold}:

<<downscale9, echo = TRUE, fig = FALSE, strip.white = false, width = 10>>=
## explore thresholds using upgrain.threshold
thresh <- upgrain.threshold(atlas.data = atlas.data,
                            cell.width = 10,
                            scales = 3,
                            thresholds = seq(0, 1, 0.01))

## see the four optional thresholds
thresh$Thresholds
@

\begin{figure}[!h]
\includegraphics[width=\linewidth]{Threshold_plots.png}
\end{figure}

\begin{figure}[!]
\includegraphics[width=\linewidth]{Threshold_maps.png}
\end{figure}

This gives two sets of plots (hit \texttt{return} or click on the plot window to see the second set). The first are a set of four plots that explore the trade-offs, and the second set are the standardised atlas data generated after applying four different threshold criteria ("All Sampled",  "All Presences",  "Gain Equals Loss" and "Sampled Only"). It is highly recommended to read the vignette “Upgraining atlas data for downscaling: threshold selection using upgrain.threshold” for more detail on creating your cross-scale standardised atlas data:

<<downscale10, eval = FALSE, strip.white = false>>=
  vignette("Upgraining", package = "downscale")
@

The user can input any threshold they wish, but for now we’ll use one of the pre-defined options “All Presences” which ensures that all presence records are maintained:

<<downscale11, echo = TRUE, eval = TRUE, fig = TRUE, strip.white = false>>=
## upgrain data (using All Presences threshold)
occupancy <- upgrain(atlas.data,
                     cell.width = 10,
                     scales = 3,
                     method = "All_Presences")
@

This creates an object of class \texttt{‘upgrain’} which can then be used directly as an input for downscaling.

\subsection{Downscaling - more detailed examples}

Fitting the downscaling models is as before, except that we no longer require to input the extent, which is passed directly from \texttt{‘upgrain’}. Let’s try the improved negative binomial model (INB):

<<downscale12, echo = TRUE, strip.white = false>>=
## if it is not already loaded, load in the package
library(downscale)

## Improved Negative Binomial model
(inb <- downscale(occupancies = occupancy,
                  model = "INB"))
@

The downscaling functions use an optimisation procedure to fit the models to the upgrained occupancy data. Suitable starting values for model parameters are automatically inputted, however if the models aren’t converging then it is possible to specify user-specific parameters. They must be in the form of a list with the same parameter names (take particular note of capitals) as the original starting parameters:

<<downscale13, echo = TRUE, strip.white = false>>=
## Specifying the starting parameters
params.new <- list("C" = 0.1, "r" = 0.00001, "b" = 0.1)
(inb.new <- downscale(occupancies = occupancy,
                      model = "INB",
                      starting_params = params.new))
@

Table 2 shows the default starting parameters implemented.

\begin{table}[!h]
\centering
\begin{tabular}{| l | l | l | l |}
\hline
\textbf{Model} & \textbf{Parameter 1} &	\textbf{Parameter 2} &	\textbf{Parameter 3} \\\hline
Nachman &	"C" = 0.01 &	"z" = 0.01 & \\	
PL &	"C" = 0.01 &	"z" = 0.01 & \\
Logis &	"C" = 0.01 &	"z" = 0.01 & \\	
Poisson &	"lambda" = 1e-8 &  &	\\
NB &	"C" = 0.01 &	"k" = 0.01 & \\	
GNB	& "C" = 0.00001 &	"z" = 1 &	"k" = 0.01 \\
INB	& "C" = 1 &	"r" = 0.01 &	"b" = 0.1 \\
FNB	& "W" = 10 &	"k" = 10 & \\	
Thomas &	"rho" = 1e-8 &	"mu" = 10 &	"sigma" = 1 \\\hline
\end{tabular}
\label{table:pars}
\end{table}

We can visually compare the two to see which has a better fit (plotting can be called directly from \texttt{predict} or through \tetxttt{plot}):

<<downscale14, echo = TRUE, fig = TRUE, strip.white = false>>=
## plot the predictions of two FNB models using predict.downscale
ind.pred <- predict(inb,
                    new.areas = c(1, 2, 5, 25, 100, 400, 1600, 6400),
                    plot = TRUE)
inb.pred.new <- predict(inb.new,
                        new.areas = c(1, 2, 5, 25, 100, 400, 1600, 6400),
                        plot = TRUE)
@

The Thomas model involves an integration process that can be time-consuming to run. For this reason the user may select tolerance during integration – the finer the tolerance, the more accurate the prediction. It can therefore be a good idea to initially try a larger tolerance value in order to ascertain if the starting parameters are likely to be correct.

<<downscale15, echo = TRUE, fig = FALSE, strip.white = false>>=
## Thomas model
thomas <- downscale(occupancies = occupancy,
                    model = "Thomas",
                    tolerance = 1e-3)

## the tolerance can also be set for the predict function
thomas.pred <- predict(thomas,
                       new.areas = c(1, 2, 5, 25, 100, 400, 1600, 6400),
                       tolerance = 1e-6)
@
<<downscale16, echo = TRUE, fig = TRUE, strip.white = false>>=
## When plotting the results we can change the graphics
plot(thomas.pred,
     col.pred = "green",
     pch = 16,
     lwd.obs = 3)
@

The Hui model is slightly different from the other downscaling models in that it does not need occupancy from multiple scales. Instead, it only takes the data from the atlas scale and uses this to calculate occupancy at finer grain sizes – in effect the hui.downscale function runs downscale and predict.downscale in one step. Therefore the input data must either be a presence-absence raster layer of the atlas data, or a data frame of cell coordinates and presence-absence data. Additionally the function requires the cell widths of the input data, along with the grain size (cell area) for which we wish to predict occupancy. These must be smaller than the cell area of the input data. Additionally, like the Thomas model, the tolerance can be specified if the results appear inaccurate (set tolerance to a smaller number) or takes extensive programming time (set tolerance to a larger number).

<<downscale17, echo = TRUE, fig = FALSE, strip.white = false>>=
## Hui model
hui <- hui.downscale(atlas.data,
                     cell.width = 10,
                     new.areas = c(1, 2, 5, 15, 50))
@
<<downscale18, echo = TRUE, fig = TRUE, strip.white = false>>=
## the output is a normal ‘predict.downscale’ object	
plot(hui)
@

\subsection{Ensemble modelling}
It is probable that the user won’t know which model will provide the most accurate predictions. Therefore there is an ensemble function that will model and predict occupancy for multiple models simultaneously. It also applies a simple model averaged prediction (the means of the log occupancies). Some or all of the models can be selected. If we don’t wish to run the Hui model input data can be a data frame of occupancies and cell areas, along with the grain sizes we wish to predict and the total extent:

<<downscale19, echo = TRUE, strip.white = false>>=
## if it is not already loaded load in the package
library(downscale)

## hypothetical occupancy data
occupancy <- data.frame(Cell.area = c(100, 400, 1600, 6400),
                        Occupancy = c(0.23, 0.56, 0.87, 1))
	
## grain sizes (cell areas) to predict
areas.pred <- c(1, 2, 5, 25, 100, 400, 1600, 6400)
@
<<downscale, echo = TRUE, fig = TRUE, strip.white = false>>=
ensemble <- ensemble.downscale(occupancy,
                               new.areas = areas.pred,
                               extent = 320000,
                               models = c("Nachman",
                                          "PL",
                                          "Logis",
                                          "GNB",
                                          "FNB"),
                               plot = TRUE)
@
<<downscale20, echo = TRUE, strip.white = false>>=
## the model averaged predictions are in grey
## predicted proportion of occupancies
ensemble$Occupancy

## predicted area of occupancies (AOO)
ensemble$AOO
@

Alternatively, the input data may be an object of class \texttt{‘upgrain’}, which also allows us to run the Hui model as long as we specify the cell width:

<<downscale21, echo = TRUE, strip.white = false>>=
## read in atlas data
data.file <- system.file("extdata", "atlas_data.txt", package = "downscale")
atlas.data <- read.table(data.file, header = TRUE)
@
\setkeys{Gin}{width=1\linewidth}
<<downscale22, echo = TRUE, fig = TRUE, strip.white = false>>=
## upgrain data (using All Presences threshold)
occupancy <- upgrain(atlas.data,
                     cell.width = 10,
                     scales = 3,
                     method = "All_Presences")
@

\setkeys{Gin}{width=0.8\linewidth}
<<downscale23, echo = TRUE, fig = TRUE, strip.white = false>>=
## ensemble modelling
ensemble <- ensemble.downscale(occupancy,
                               new.areas = areas.pred,
                               cell.width = 10,
                               models = c("Nachman",
                                          "PL",
                                          "Logis",
                                          "GNB",
                                          "FNB",
                                          "Hui"),
                               plot = TRUE)
@

If we want to run all ten models we can specify \texttt{models = "all"}. Once again, we can set the tolerance values for the modelling (\texttt{tolerance\_mod}) and prediction (\texttt{tolerance\_pred}) of the Thomas model and the Hui model (\texttt{tolerance\_hui}) to improve processing times or accuracy.

\setkeys{Gin}{width=1\linewidth}
<<downscale24, echo = TRUE, fig = TRUE, strip.white = false>>=
ensemble <- ensemble.downscale(occupancy,
                               new.areas = areas.pred,
                               cell.width = 10,
                               models = "all",
                               tolerance_mod = 1e-3,
                               plot = TRUE)
@

We can also specify the starting parameters for specific models. For each model the starting parameters should be in the form of a list as before, and each model list is an item in a combined list:

<<downscale25, echo = TRUE, strip.white = false>>=
## Specifying starting parameters for Nachman and GNB models
new.params <- list(Nachman = list("C" = 0.1, "z" = 0.01),
                   GNB = list("C" = 0.1, "z" = 1, "k" = 0.01))
new.params
@

<<downscale26, echo = TRUE, fig = TRUE, strip.white = false>>=
ensemble <- ensemble.downscale(occupancies = occupancy,
                               new.areas = c(1, 2, 5, 15, 50, 100, 400, 1600,
                                             6400),
                               cell.width = 10,
                               models = "all",
                               tolerance_mod = 1e-3,
                               starting_params = new.params,
                               plot = TRUE)
@

\subsection{Creating atlas data from point records}

It may be that instead of having pre-existing atlas data, we may need to create our own coarse-scale data from point records (for example herbarium records or GBIF data).

The grain size (cell width) needs to be carefully chosen so that we can best meet the assumption that all cells have been sampled. If there are cells or regions where we do not expect this to be the case it is best to change these to NA’s rather than assign them as absences.

The library \texttt{spocc} will automatically harvest GBIF data for a desired species for a specified region.

<<downscale27, echo = FALSE, eval = FALSE, strip.white = false>>=
install.packages("spocc", repos = "http://cran.us.r-project.org")
@
<<downscale, echo = TRUE, strip.white = false>>=
library(spocc)
library(downscale)
@

We’ll get the records for the chalkhill blue (Polyommatus coridon), a species with a patchy breeding distribution in the UK.

<<downscale28, echo = TRUE, strip.white = false>>=
records <- occ(query = "Polyommatus coridon",
               from = "gbif",
               limit=10000,
               gbifopts = list(country ="GB"))
records <- occ_to_sp(records,
                     "+proj=longlat +datum=WGS84",
                     just_coords = TRUE)
records <- spTransform(records,
                       CRS("+proj=lcc +lat_1=49.8333339 +lat_2=51.16666733333333
                           +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.01256 
                           +y_0=5400088.4378 +ellps=intl +units=km +no_defs"))
@

We can have a quick look at the point records if we like.

<<downscale29, echo = TRUE, strip.white = false>>=
records.coords <- records@coords
@
\setkeys{Gin}{width=0.8\linewidth}
<<downscale30, echo = TRUE, fig = TRUE, strip.white = false>>=
plot(records.coords)
@

Now we have to convert these points in to a coarse-scale raster. The simplest method is to bound our raster by the limits of the location coordinates. Careful thought must also be put in to the grain size. It must be large enough that we are confident it is an accurate representation of presence-absence, but also small enough to allow upgraining to give at least three spatial scales worth of occupancy data. Here, as the UK is generally well-sampled, we will set a grain size of 20 km width (400 $km^{2}$), which will still comfortably allow us to upgrain to give three scales (400, 1600, 6400 $km^{2}$).

<<downscale31, echo = TRUE, strip.white = false>>=
gbif_raster <- raster(xmn = min(records.coords[, "longitude"]),
                      xmx = max(records.coords[, "longitude"]),
                      ymn = min(records.coords[, "latitude"]),
                      ymx = max(records.coords[, "latitude"]),
                      res = 20)
gbif_raster <- rasterize(records, gbif_raster, field = 1)

## convert cells with NA (no records) to 0
gbif_raster[is.na(gbif_raster)] <- 0
@
<<downscale32, echo = TRUE, fig = TRUE, strip.white = false>>=
plot(gbif_raster)
@

As our area is rectangular we should not be too worried about setting our thresholds for upgraining, and so we can choose the “All Sampled” option to maintain all data.

<<downscale33, echo = TRUE, fig = TRUE, strip.white = false>>=
occupancy <- upgrain(gbif_raster,
                     scales = 2,
                     method = "All_Sampled")
@
<<downscale34, echo = TRUE, strip.white = false>>=
## We can see there has not been much increase in extent
occupancy$occupancy.orig[1, 2]
occupancy$extent.stand
@

Now we can run our ensemble downscaling models:

\setkeys{Gin}{width=1\linewidth}
<<downscale35, echo = TRUE, fig = TRUE, strip.white = false>>=
ensemble <- ensemble.downscale(occupancy,
                               models = "all",
                               new.areas = c(1, 10, 100, 400, 1600, 6400),
                               tolerance_mod = 1e-3)
@

The INB model has not converged satisfactorily and thrown up a warning message (it has predicted a 0 at the finest grain size which we know to be impossible). We can try tweaking it’s starting parameters to see if we can get a better fit:

<<downscale36, echo = TRUE, fig = TRUE, strip.white = false>>=
ensemble <- ensemble.downscale(occupancy,
                               models = "all",
                               new.areas = c(1, 10, 100, 400, 1600, 6400),
                               tolerance_mod = 1e-3,
                               starting_params = list(INB = list(C = 10, 
                                                                 r = 0.01, 
                                                                 b = 0.1)))
@

In this case we have simply drawn a rectangle around our points, but perhaps we have a better idea of the possible range limits of the species. In our case, it is probably sensible to set the extent as mainland UK. There is a shapefile which we can load in and set as our extent.

<<downscale37, echo = TRUE,strip.white = false>>=
uk <- system.file("extdata", "UK.shp", package = "downscale")
uk <- shapefile(uk)

## coverts units to be the same as our GBIF data (in km)
uk <- spTransform(uk,
                  CRS("+proj=lcc +lat_1=49.8333339 +lat_2=51.16666733333333 
                      +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.01256 
                      +y_0=5400088.4378 +ellps=intl +units=km +no_defs"))
@

\setkeys{Gin}{width=0.8\linewidth}
<<downscale38, echo = TRUE, fig = TRUE, strip.white = false>>=
## plot our GBIF records on top of the UK polygon
plot(uk)
plot(records, add = TRUE)
@

Now, we make our raster to be the same extent as the UK polygon, and then mask the raster file with the UK polygon so that any cells outside this polygon are assigned as NA (unsampled cells):

<<downscale39, echo = TRUE, strip.white = false>>=
gbif_raster <- raster(ext = extent(uk),
                      res = 20)
gbif_raster <- rasterize(records, gbif_raster, field = 1)
gbif_raster[is.na(gbif_raster)] <- 0
gbif_raster <- mask(gbif_raster, uk)
@
<<downscale40, echo = TRUE, fig = TRUE, strip.white = false>>=
plot(gbif_raster)
plot(uk, add = TRUE)
@

Now, we just upgrain and downscale as before:

<<downscale41, echo = TRUE, fig = TRUE, strip.white = false>>=
occupancy <- upgrain(gbif_raster,
                     scales = 2,
                     method = "All_Sampled")
@
\setkeys{Gin}{width=1\linewidth}
<<downscale42, echo = TRUE, fig = TRUE, strip.white = false>>=
ensemble.downscale(occupancy,
                   models = "all",
                   new.areas = c(1, 10, 100, 400, 1600, 6400),
                   tolerance_mod = 1e-3,
                   starting_params = list(INB = list(C = 10,
                                                     r = 0.01,
                                                     b = 0.1)))
@

If we want to compare occupancy between the two raster extents we must compare the converted area of occupancies (AOO), no proportion of occupancy (which are the proportions for different extents). In this case the estimates for grain sizes of 1 km and 10 km from the bounded rectangle (AOO = 627 $km^{2}$ and 3052 $km^{2}$) are larger than the estimates using the full mainland UK (AOO = 556 $km^{2}$ and 2635 $km^{2}$) highlighting the importance of this decision.

\section{Bibliography}

Azaele, S., S. J. Cornell, and W. E. Kunin. 2012. Downscaling species occupancy from coarse spatial scales. Ecological Applications 22:1004–14.

Barwell, L. J., S. Azaele, W. E. Kunin, and N. J. B. Isaac. 2014. Can coarse-grain patterns in insect atlas data predict local occupancy? Diversity and Distributions 20:895–907.

Hartley, S., and W. E. Kunin. 2003. Scale dependence of rarity, extincition risk, and conservation priority. Conservation Biology 17:1–12.

Hui, C., M. A. McGeoch, B. Reyers, P. C. le Roux, M. Greve, and S. L. Chown. 2009. Extrapolating population size from the occupancy-abundance relationship and the scaling pattern of occupancy. Ecological Applications 19:2038–2048.

Hui, C., M. A. McGeoch, and M. Warren. 2006. A spatially explicit approach to estimating species occupancy and spatial correlation. Journal of Animal Ecology 75:140–147.

IUCN. 2014. Guidelines for using the IUCN Red List categories and criteria.

Kunin, W. E. 1998. Extrapolating species abundance across spatial scales. Science 281:1513–1515.

\end{document}